{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44ca032",
   "metadata": {},
   "source": [
    "# üß† Workshop: Building Blocks for AI Agents\n",
    "\n",
    "## NLP Pipeline + Probabilistic Language Models (90-Minute Team Lab)\n",
    "\n",
    "**Objective:**\n",
    "Work in teams of 3 to build a small NLP pipeline and implement unigram and bigram models, culminating in estimating sentence probabilities. Submit your completed Jupyter Notebook via a GitHub link (with `.git` at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c538b6",
   "metadata": {},
   "source": [
    "## Group 8\n",
    "\n",
    "### Eris Leksi\n",
    "\n",
    "### Erica Holden\n",
    "\n",
    "### Reham Abuarqoub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a6f0",
   "metadata": {},
   "source": [
    "## Part 1 ‚Äì NLP Pipeline\n",
    "\n",
    "### Step 1: Select and Load a Corpus\n",
    "\n",
    "Select a corpus from `nltk`, or upload your own text documents. Ensure your vocabulary size exceeds 2000 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b931a",
   "metadata": {},
   "source": [
    "# Sustainable NLP: Optimization & Recommendation Engine  \n",
    "### Using the \"Awesome ChatGPT Prompts\" Dataset\n",
    "\n",
    "This notebook demonstrates a pipeline to analyze and optimize long prompts from the **Awesome ChatGPT Prompts** dataset.  \n",
    "The goal is to reduce prompt length and token usage without sacrificing semantic meaning ‚Äî contributing to energy-efficient AI inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b618c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 203 prompts.\n",
      "Example prompt:\n",
      " Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Awesome ChatGPT Prompts dataset\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\")\n",
    "\n",
    "# Extract the prompt text field from the dataset\n",
    "prompts = [item['prompt'] for item in dataset]\n",
    "\n",
    "print(f\"Loaded {len(prompts)} prompts.\")\n",
    "print(\"Example prompt:\\n\", prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983cf74",
   "metadata": {},
   "source": [
    "### Corpus Overview\n",
    "\n",
    "We have loaded a collection of user-generated prompts designed to engage ChatGPT.  \n",
    "These prompts vary in length and complexity, making them ideal for testing prompt optimization techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a49c6c",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** This corpus is pre-tokenized and covers multiple topics. It‚Äôs a good fit to get us above the 2,000-word vocabulary requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb1158",
   "metadata": {},
   "source": [
    "### Step 2: Collect and Preprocess Documents\n",
    "\n",
    "Convert your corpus into tokens and compute the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc40f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt length statistics:\n",
      "       Word Count\n",
      "count  203.000000\n",
      "mean    82.088670\n",
      "std     35.695938\n",
      "min     20.000000\n",
      "25%     62.000000\n",
      "50%     75.000000\n",
      "75%     88.500000\n",
      "max    307.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate word counts for each prompt\n",
    "word_counts = [len(p.split()) for p in prompts]\n",
    "df_stats = pd.DataFrame(word_counts, columns=['Word Count'])\n",
    "\n",
    "print(\"Prompt length statistics:\")\n",
    "print(df_stats.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab33b38",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We calculated word counts for each prompt to understand their length distribution. This helps with optimizing prompt design and preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d5c29",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "Before analyzing the prompts, we clean and normalize the text by removing extra spaces and unwanted characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69341f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First cleaned prompt:\n",
      "Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "cleaned_prompts = [clean_text(p) for p in prompts]\n",
    "\n",
    "print(f\"First cleaned prompt:\\n{cleaned_prompts[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86651d",
   "metadata": {},
   "source": [
    "We applied basic text cleaning to each prompt by removing extra spaces and trimming whitespace for consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1b0c8",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Vocabulary size is important‚Äîit determines the richness of our model. Models trained on small vocabularies can't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a46555",
   "metadata": {},
   "source": [
    "### Step 3: Implement Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86fe58",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "We tokenize prompts using the OpenAI `tiktoken` tokenizer to estimate token counts, which correspond directly to inference costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a7f44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in first prompt:\n",
      "[52157, 499, 527, 459, 10534, 35046, 16131, 51920, 449, 6968, 264, 7941, 5226, 369, 264, 18428, 50596, 13, 578, 16945, 374, 311, 3665, 6743, 389, 279, 18428, 11, 3339, 1124, 34898, 320, 898, 8, 311, 5127, 11, 47005, 320, 2039, 8, 1193, 311, 279, 1732, 889, 27167, 279, 5226, 11, 323, 311, 1797, 1268, 1690, 3115, 279, 1984, 574, 6177, 13, 8000, 264, 22925, 488, 7941, 5226, 369, 420, 7580, 11, 2737, 279, 5995, 5865, 323, 38864, 369, 32145, 279, 5300, 9021, 13, 5321, 3493, 279, 2082, 323, 904, 9959, 41941, 311, 6106, 264, 2867, 8830, 315, 279, 8292, 13]\n",
      "Token count in first prompt: 100\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokenized_prompts = [tokenizer.encode(p) for p in cleaned_prompts]\n",
    "\n",
    "print(f\"Tokens in first prompt:\\n{tokenized_prompts[0]}\")\n",
    "print(f\"Token count in first prompt: {len(tokenized_prompts[0])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c09c0",
   "metadata": {},
   "source": [
    "Each cleaned prompt was tokenized using OpenAI‚Äôs `tiktoken` library to prepare for language model analysis. This step converts text into a sequence of integer tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3cee35",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** A simple regex tokenizer gives us control‚Äîthis is useful when we need to understand every processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e2bf2",
   "metadata": {},
   "source": [
    "### Optimization Objective\n",
    "\n",
    "Our goal is to reduce token counts by rewriting verbose prompts while preserving semantic meaning.  \n",
    "We will implement a simple rule-based rewriting engine as a prototype.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315d4f5",
   "metadata": {},
   "source": [
    "### Step 4: Normalization, Stemming, and Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2ca2b",
   "metadata": {},
   "source": [
    "\n",
    "We further preprocess by normalizing case, stemming words using PorterStemmer, and removing stopwords to reduce vocabulary size and focus on core semantics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa6a35f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed first prompt tokens:\n",
      "['imagin', 'experienc', 'ethereum', 'develop', 'task', 'creat', 'smart', 'contract', 'blockchain', 'messeng', 'object', 'save', 'messag', 'blockchain', 'make', 'readabl', 'public', 'everyon', 'writabl', 'privat', 'person', 'deploy', 'contract', 'count', 'mani', 'time', 'messag', 'updat', 'develop', 'solid', 'smart', 'contract', 'purpos', 'includ', 'necessari', 'function', 'consider', 'achiev', 'specifi', 'goal', 'pleas', 'provid', 'code', 'relev', 'explan', 'ensur', 'clear', 'understand', 'implement']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK data if not already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    filtered = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    return filtered\n",
    "\n",
    "processed_prompts = [preprocess_text(p) for p in cleaned_prompts]\n",
    "\n",
    "print(f\"Processed first prompt tokens:\\n{processed_prompts[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab374a2",
   "metadata": {},
   "source": [
    "We applied NLTK-based preprocessing by removing stopwords and stemming tokens. This reduces noise and standardizes the vocabulary for language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25579af",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Normalization makes the data more consistent and shrinks the vocabulary. This is essential for estimating reliable probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0a29b",
   "metadata": {},
   "source": [
    "## Part 2 ‚Äì Probabilistic Language Models\n",
    "\n",
    "### üìò Unigram Model\n",
    "\n",
    "A **Unigram Model** is a type of probabilistic language model that assumes each word in a sentence is **independent** of the words that came before it.\n",
    "\n",
    "The probability of a sequence of words $w_1, w_2, ..., w_n$ is calculated as:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "To estimate $P(w_i)$, we use the **Maximum Likelihood Estimate (MLE)**:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\sum_{j} \\text{count}(w_j)}\n",
    "$$\n",
    "\n",
    "where $j$ is the total number of words in the corpus.\n",
    "\n",
    "This is a strong simplification, but it provides a foundational baseline and helps reduce data sparsity in low-resource environments.\n",
    "\n",
    "Here's how to implement it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e617ea2",
   "metadata": {},
   "source": [
    "\n",
    "We now build foundational probabilistic language models (unigram and bigram) to estimate the probability of prompts, which helps evaluate fluency and support prompt optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32eb6d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique unigrams: 2016\n",
      "Sample unigram probabilities:\n",
      "[('imagin', 0.0007746790615316512), ('experienc', 0.0007746790615316512), ('ethereum', 0.00011066843736166445), ('develop', 0.005533421868083223), ('task', 0.0021027003098716248), ('creat', 0.005312084993359893), ('smart', 0.00033200531208499334), ('contract', 0.00033200531208499334), ('blockchain', 0.0002213368747233289), ('messeng', 0.00011066843736166445)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten list of processed tokens to get unigram counts\n",
    "all_tokens = [token for prompt in processed_prompts for token in prompt]\n",
    "unigram_counts = Counter(all_tokens)\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "# Calculate unigram probabilities\n",
    "unigram_probs = {token: count / total_unigrams for token, count in unigram_counts.items()}\n",
    "\n",
    "print(f\"Total unique unigrams: {len(unigram_probs)}\")\n",
    "print(f\"Sample unigram probabilities:\\n{list(unigram_probs.items())[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e9089e",
   "metadata": {},
   "source": [
    "We computed unigram frequencies and their probabilities using Maximum Likelihood Estimation (MLE). This forms the basis for our simplest probabilistic language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bde8da",
   "metadata": {},
   "source": [
    "##### üìò Why Are Unigram Probabilities So Low?\n",
    "\n",
    "Unigram probabilities represent the **relative frequency** of individual words in the entire corpus:\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i)}{\\text{total number of tokens in the corpus}}\n",
    "$$\n",
    "\n",
    "In our case, the total number of tokens is quite large:\n",
    "\n",
    "- **Total tokens:** 1,178,604  \n",
    "- **Unique words (vocabulary size):** 67,151\n",
    "\n",
    "Even if a word appears frequently, its probability will still be small relative to the total number of tokens.\n",
    "\n",
    "For example:\n",
    "- `\"bank\"` appears quite often, yet its probability is only **0.00493**, or about **0.5%** of the total words.\n",
    "- `\"citibank\"` appears only a few times, resulting in a much smaller probability of **0.00005**.\n",
    "\n",
    "These small values are expected when:\n",
    "- The corpus is **large and diverse** (like Reuters).\n",
    "- Many words appear **only once or twice**, which is common in natural language (known as Zipf's Law).\n",
    "\n",
    "**Conclusion:**  \n",
    "Low unigram probabilities do **not** indicate an error‚Äîthey reflect a realistic distribution of word frequencies across a large corpus. This also highlights the need for smoothing when building more complex language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a95505",
   "metadata": {},
   "source": [
    "### üìò Chain Rule with Unigrams\n",
    "\n",
    "Using the **Chain Rule**, we estimate the probability of a sequence:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "This is a simplifying assumption of complete independence (unrealistic but foundational)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb0cee",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Unigram models assume word independence‚Äîuseful but limited since word order is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12b2ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram sequence probability for first prompt: 2.525468482982433e-153\n"
     ]
    }
   ],
   "source": [
    "def unigram_sequence_prob(sequence, unigram_prob_dict):\n",
    "    prob = 1.0\n",
    "    for token in sequence:\n",
    "        prob *= unigram_prob_dict.get(token, 1e-8)  # Smoothing for unseen words\n",
    "    return prob\n",
    "\n",
    "# Example on first processed prompt\n",
    "seq_prob = unigram_sequence_prob(processed_prompts[0], unigram_probs)\n",
    "print(f\"Unigram sequence probability for first prompt: {seq_prob}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030c0fc",
   "metadata": {},
   "source": [
    "We calculate the probability of a word sequence under the unigram model by multiplying individual word probabilities, applying smoothing for unseen tokens.\n",
    "\n",
    "The extremely small probability value (e.g., 2.5e-153) reflects the product of many small individual word probabilities. Since the unigram model multiplies the probabilities of each word assuming independence, the more words in a prompt, the smaller the overall sequence probability becomes. This is expected and highlights why smoothing and more complex models (like bigrams) are needed for practical language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983954cf",
   "metadata": {},
   "source": [
    "##### üìò Why Is the Sentence Probability So Low?\n",
    "\n",
    "The calculated **unigram sentence probability** is:\n",
    "\n",
    "```python\n",
    "2.382179640797073e-37\n",
    "````\n",
    "\n",
    "This number is extremely small‚Äîbut **that‚Äôs expected** for long sentences under a unigram model. Here's why:\n",
    "\n",
    "\n",
    "##### üî¢ Corpus Statistics\n",
    "\n",
    "* **Total number of tokens:** 1,178,604\n",
    "* **Vocabulary size (unique tokens):** 67,151\n",
    "\n",
    "##### üìâ How the Unigram Model Works\n",
    "\n",
    "The unigram model computes sentence probability as the **product of individual word probabilities**:\n",
    "\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = \\prod_{i=1}^{n} P(w_i)\n",
    "$$\n",
    "\n",
    "Each word typically has a probability between 0.00001 and 0.01. When multiplying **10‚Äì20 small numbers together**, the final result becomes **exponentially smaller**, approaching zero for longer sentences.\n",
    "\n",
    "##### üß™ Impact of Preprocessing (Step 4)\n",
    "\n",
    "The normalization step involves:\n",
    "\n",
    "* Lowercasing\n",
    "* **Stop word removal** (e.g., \"the\", \"of\", \"for\", \"said\")\n",
    "* **Stemming** (e.g., \"management\" ‚Üí \"manag\")\n",
    "* **Punctuation removal**\n",
    "\n",
    "This reduces the number of words used in the calculation. While this makes the vocabulary smaller and more manageable, it also means:\n",
    "\n",
    "* **Common but removed words** (like \"the\") don‚Äôt contribute to the probability.\n",
    "* **Stemmed forms** may not match original unigrams perfectly (e.g., ‚Äúsino-chilean‚Äù becomes `sinochilean` or `sino` and `chilean`, depending on the tokenizer).\n",
    "\n",
    "So even though the sentence appears long, **only 7‚Äì12 stemmed and filtered tokens** may remain after preprocessing‚Äîyet each one still has a very small individual probability.\n",
    "\n",
    "##### ‚úÖ Key Takeaways\n",
    "\n",
    "* Low sentence probabilities are **normal** in unigram models, especially for longer sentences.\n",
    "* The **multiplicative nature** of probability and the **sparsity of natural language** lead to very small final values.\n",
    "* These limitations are one reason why more advanced models (like bigrams or neural LMs) are needed for realistic NLP applications.\n",
    "\n",
    "You can inspect the intermediate tokens like this:\n",
    "\n",
    "```python\n",
    "print(normalize(simple_tokenizer(sentence)))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d7d38",
   "metadata": {},
   "source": [
    "### üìò Bigram Model with MLE ‚Äì Mathematical Explanation\n",
    "\n",
    "The **Bigram Model** assumes the current word depends only on the previous word.\n",
    "The MLE (Maximum Likelihood Estimate) for a bigram $(w_{i-1}, w_i)$ is:\n",
    "$$\n",
    "P(w_i | w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47092802",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** This simple multiplication illustrates the chain rule, but we‚Äôll soon see how to improve this with context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d74b7c",
   "metadata": {},
   "source": [
    "### üìò Sentence Probability with Bigram Model ‚Äì Mathematical Explanation\n",
    "\n",
    "Using the bigram model and chain rule:\n",
    "$$\n",
    "P(w_1, w_2, ..., w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_2) \\cdots P(w_n | w_{n-1})\n",
    "$$\n",
    "This models **local dependencies** between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3371af74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bigram probabilities for 'imagin':\n",
      "[('experienc', 0.14285714285714285), ('captiv', 0.14285714285714285), ('depend', 0.14285714285714285), ('descript', 0.2857142857142857), ('work', 0.14285714285714285)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "bigram_counts = defaultdict(Counter)\n",
    "unigram_counts_for_bigram = Counter()\n",
    "\n",
    "for tokens in processed_prompts:\n",
    "    for i in range(len(tokens)):\n",
    "        unigram_counts_for_bigram[tokens[i]] += 1\n",
    "        if i > 0:\n",
    "            bigram_counts[tokens[i-1]][tokens[i]] += 1\n",
    "\n",
    "# Calculate bigram probabilities with MLE\n",
    "bigram_probs = {}\n",
    "for w1 in bigram_counts:\n",
    "    bigram_probs[w1] = {}\n",
    "    total_count = sum(bigram_counts[w1].values())\n",
    "    for w2 in bigram_counts[w1]:\n",
    "        bigram_probs[w1][w2] = bigram_counts[w1][w2] / total_count\n",
    "\n",
    "print(f\"Sample bigram probabilities for '{list(bigram_probs.keys())[0]}':\")\n",
    "print(list(bigram_probs[list(bigram_probs.keys())[0]].items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f36c1",
   "metadata": {},
   "source": [
    "We computed bigram counts and probabilities using Maximum Likelihood Estimation (MLE). This model estimates the probability of each word given the previous word, capturing local word dependencies.\n",
    "\n",
    "The output shows the probability distribution of words that follow the token `'imagin'` in the corpus. For example, `'descript'` follows `'imagin'` about 28.57% of the time, while `'experienc'`, `'captiv'`, `'depend'`, and `'work'` each follow with roughly 14.29% probability. These probabilities reflect how often each bigram occurred relative to all bigrams starting with `'imagin'`, capturing local context dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a63c06",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Bigram probabilities model word context, capturing more meaning than unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21066",
   "metadata": {},
   "source": [
    "### Sentence Probability with Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bacb2946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram sequence probability for first prompt: 6.411700207683277e-42\n"
     ]
    }
   ],
   "source": [
    "def bigram_sequence_prob(sequence, unigram_prob_dict, bigram_prob_dict):\n",
    "    if not sequence:\n",
    "        return 0\n",
    "    prob = unigram_prob_dict.get(sequence[0], 1e-8)  # P(w1)\n",
    "    for i in range(1, len(sequence)):\n",
    "        w1, w2 = sequence[i-1], sequence[i]\n",
    "        prob *= bigram_prob_dict.get(w1, {}).get(w2, 1e-8)  # P(w_i | w_{i-1})\n",
    "    return prob\n",
    "\n",
    "# Example on first processed prompt\n",
    "seq_prob_bigram = bigram_sequence_prob(processed_prompts[0], unigram_probs, bigram_probs)\n",
    "print(f\"Bigram sequence probability for first prompt: {seq_prob_bigram}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f527f2",
   "metadata": {},
   "source": [
    "This function computes the probability of a word sequence using the bigram model by multiplying the conditional probabilities of each word given its predecessor. It applies a small fallback probability for unseen bigrams to avoid zero probability issues.\n",
    "\n",
    "The bigram sequence probability (e.g., 6.4e-42) is larger than the unigram probability but still very small due to multiplying many probabilities. This reflects how the bigram model captures local word dependencies, making it a more realistic estimate than the unigram model, though sequence probabilities naturally decrease with longer sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447d671",
   "metadata": {},
   "source": [
    "**üë®‚Äçüè´ Professor Talking Point:** Estimating sentence probability using bigrams shows how sequence information improves prediction power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b05938",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Loaded and preprocessed a real-world prompt corpus.\n",
    "- Implemented tokenization, normalization, stemming, and stopword removal.\n",
    "- Built unigram and bigram probabilistic language models using MLE.\n",
    "- Calculated sequence probabilities using chain rule assumptions.\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "- Implement smoothing techniques (Laplace, Kneser-Ney) to handle zero probabilities.\n",
    "- Evaluate prompt optimization by scoring original vs. rewritten prompts.\n",
    "- Integrate semantic similarity models to preserve meaning.\n",
    "- Explore neural paraphrasing models with energy-aware constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebc0fb",
   "metadata": {},
   "source": [
    "## Prompt Rewriting & Semantic Evaluation\n",
    "\n",
    "To evaluate sustainability and clarity, we rewrite prompts using simple heuristics and measure their semantic similarity to the originals using sentence embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f8e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original: Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.\n",
      "Rewritten: Imagine you are an experienced Ethereum developer tasked with creating a smart contract for a blockchain messenger. The objective is to save messages on the blockchain, making them readable (public) to everyone, writable (private) only to the person who deployed the contract, and to count how many times the message was updated. Develop a Solidity smart contract for this purpose, including the necessary functions and considerations for achieving the specified goals. Please provide the code and any relevant explanations to ensure a clear understanding of the implementation.\n",
      "\n",
      "Original: Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they‚Äôre not competing articles. Split the outline into part 1 and part 2.\n",
      "Rewritten: Using WebPilot, create an outline for an article that will be 2,000 words on the keyword 'Best SEO prompts' based on the top 10 results from Google. Include every relevant heading possible. Keep the keyword density of the headings high. For each section of the outline, include the word count. Include FAQs section in the outline too, based on people also ask section from Google for the keyword. This outline must be very detailed and comprehensive, so that I can create a 2,000 word article from it. Generate a long list of LSI and NLP keywords related to my keyword. Also include any other words related to the keyword. Give me a list of 3 relevant external links to include and the recommended anchor text. Make sure they‚Äôre not competing articles. Split the outline into part 1 and part 2.\n",
      "\n",
      "Original: I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\n",
      "Rewritten: Please act as a linux terminal. I will type commands and you will reply with what the terminal should show. Please only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\n",
      "\n",
      "Original: I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is \"istanbulu cok seviyom burada olmak cok guzel\"\n",
      "Rewritten: Please act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. Please replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. Please only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is \"istanbulu cok seviyom burada olmak cok guzel\"\n",
      "\n",
      "Original: I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conversation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is \"Hi\"\n",
      "Rewritten: Please act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. Please only reply as the interviewer. Do not write all the conversation at once. Please only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is \"Hi\"\n",
      "\n",
      "Original: I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is console.log(\"Hello World\");\n",
      "Rewritten: Please act as a javascript console. I will type commands and you will reply with what the javascript console should show. Please only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is console.log(\"Hello World\");\n"
     ]
    }
   ],
   "source": [
    "# Heuristic function: simplify wording & remove filler\n",
    "def rewrite_prompt(prompt):\n",
    "    prompt = re.sub(r\"^Act as an? \", \"\", prompt)  # remove \"Act as a\"\n",
    "    prompt = prompt.replace(\"I want you to\", \"Please\")\n",
    "    prompt = prompt.replace(\"could you\", \"can you\")\n",
    "    prompt = prompt.replace(\"would you\", \"can you\")\n",
    "    prompt = prompt.replace(\"Write a\", \"Give a\")\n",
    "    return prompt.strip()\n",
    "\n",
    "rewritten_prompts = [rewrite_prompt(p) for p in cleaned_prompts]\n",
    "\n",
    "# Show comparison\n",
    "for i in range(6):\n",
    "    print(f\"\\nOriginal: {cleaned_prompts[i]}\")\n",
    "    print(f\"Rewritten: {rewritten_prompts[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f55ad",
   "metadata": {},
   "source": [
    "### Explanation of Prompt Rewriting\n",
    "\n",
    "In this step, we applied a simple heuristic function to streamline and simplify prompt wording. The function removes common filler phrases such as ‚ÄúAct as a‚Äù, and replaces verbose expressions like ‚ÄúI want you to‚Äù with shorter alternatives like ‚ÄúPlease.‚Äù This approach helps make prompts more concise without changing their core meaning.\n",
    "\n",
    "\n",
    "### Output Interpretation\n",
    "\n",
    "The output displays pairs of original and rewritten prompts side-by-side. In some cases, the rewritten prompt remains the same because no matching filler phrases were found to simplify. In others, subtle wording changes make the prompt shorter and clearer‚Äîfor example, changing ‚ÄúI want you to act as a linux terminal‚Äù to ‚ÄúPlease act as a linux terminal.‚Äù This demonstrates how small edits can contribute to reducing token usage while preserving the intent, which is key for sustainable AI and energy-efficient prompting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71c110",
   "metadata": {},
   "source": [
    "### Semantic Similarity Using Sentence Embeddings\n",
    "\n",
    "We use pretrained Sentence Transformers to compute cosine similarity between original and rewritten prompts. A high score (~1.0) indicates semantic preservation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24ea3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score 1: 1.0000\n",
      "Similarity Score 2: 1.0000\n",
      "Similarity Score 3: 0.9291\n",
      "Similarity Score 4: 0.9086\n",
      "Similarity Score 5: 0.9186\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute embeddings\n",
    "original_embeddings = model.encode(cleaned_prompts[:100], convert_to_tensor=True)\n",
    "rewritten_embeddings = model.encode(rewritten_prompts[:100], convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(original_embeddings, rewritten_embeddings).diagonal()\n",
    "\n",
    "# Display some similarity scores\n",
    "for i in range(5):\n",
    "    print(f\"Similarity Score {i+1}: {cosine_scores[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141898b7",
   "metadata": {},
   "source": [
    "### Explanation of Semantic Similarity Calculation\n",
    "\n",
    "In this step, we evaluated how well the rewritten prompts preserved the meaning of the original prompts. Using the `sentence-transformers` model (`all-MiniLM-L6-v2`), we converted both sets of prompts into dense vector embeddings that capture their semantic content.\n",
    "\n",
    "We then computed the cosine similarity between each original prompt embedding and its rewritten counterpart. Cosine similarity measures how close two vectors are in meaning, with values ranging from -1 (completely different) to 1 (identical).\n",
    "\n",
    "### Output Interpretation\n",
    "\n",
    "The similarity scores printed show mostly very high values (close to 1.0), indicating that the rewritten prompts maintain strong semantic equivalence with the originals. For example:\n",
    "\n",
    "- Scores of **1.0000** mean the rewritten prompt is nearly identical in meaning.\n",
    "- Scores around **0.9** indicate minor wording differences but overall preserved intent.\n",
    "\n",
    "This confirms that our heuristic rewriting simplifies prompts without significantly changing their meaning, supporting energy-efficient prompt optimization without sacrificing clarity or correctness. Even though this might not be enough, to us is a great first step to take.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c90f71",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "We achieved semantic similarity scores above 0.9 for most rewritten prompts, indicating that our heuristic edits preserved meaning. This validates the potential for lightweight prompt optimization methods that save tokens while remaining faithful to intent. Even though this might not be enough, to us is a great first step to take.\n",
    "\n",
    "This technique can be used in Sustainable AI to:\n",
    "- Compress long prompts without losing clarity\n",
    "- Maintain meaning with fewer tokens (compute-efficient)\n",
    "- Optimize prompt quality for different model contexts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d6b238",
   "metadata": {},
   "source": [
    "## Part 3: The Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac59089",
   "metadata": {},
   "source": [
    "\n",
    "One team member must push the final notebook to GitHub and send the `.git` URL to the instructor before the end of class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7a67a",
   "metadata": {},
   "source": [
    "## üß† Learning Objectives\n",
    "- Implement the foundations of **Probabilistic Language Models** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## üß© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(20 min)* ‚Äì Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(65 min)* ‚Äì NLP Pipeline and four Probabilistic Language Model method implementations + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(5 min)* ‚Äì Teams commit and push the one notebook. **Make sure to include your names so it is easy to identify the team that developed the code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(1 min)* ‚Äì Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Probabilistic Language Models Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## üíª Submission Checklist\n",
    "- ‚úÖ `ProbabilisticLanguageModels.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, Inverted Index and the four methods.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** (1-2 per concept)\n",
    "- ‚úÖ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ‚úÖ GitHub Repo:\n",
    "  - Public repo named `ProbabilisticLanguageModels`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b737ca3",
   "metadata": {},
   "source": [
    "## üß≠ Conclusion\n",
    "\n",
    "Today you‚Äôve constructed your own basic language model. Next class, we‚Äôll expand these ideas to explore **Large Language Models (LLMs)**‚Äîlike ChatGPT‚Äîwhich learn patterns over **massive corpora** using **deep neural networks** instead of just counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
